import streamlit as st
import pandas as pd
import plotly.express as px
import json
import re
from datetime import datetime
import logging

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
st.set_page_config(layout="wide", page_title="ĞĞ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ¾ Ğ“Ğ°Ğ·Ğ¿Ñ€Ğ¾Ğ¼Ğ±Ğ°Ğ½ĞºĞµ")

# Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
st.title("ĞĞ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ² Ğ¾ Ğ“Ğ°Ğ·Ğ¿Ñ€Ğ¾Ğ¼Ğ±Ğ°Ğ½ĞºĞµ")

# Ğ¡Ğ°Ğ¹Ğ´Ğ±Ğ°Ñ€ Ğ´Ğ»Ñ Ğ²Ğ¸Ğ´Ğ¶ĞµÑ‚Ğ¾Ğ²
st.sidebar.header("ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¸ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")

# Ğ’Ğ¸Ğ´Ğ¶ĞµÑ‚Ñ‹ Ğ´Ğ»Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²
uploaded_csv = st.sidebar.file_uploader("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ CSV Ñ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ğ¼Ğ¸", type=['csv'])
uploaded_json = st.sidebar.file_uploader("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ JSON Ñ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°", type=['json'])

# ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ»Ğ¾Ğ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ
logging.basicConfig(filename='app_errors.log', level=logging.ERROR,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ´Ğ»Ñ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ñ Ğ²ĞµÑĞ°Ğ¼Ğ¸
SENTIMENT_LEXICON = {
    'positive': {
        'Ğ¾Ñ‚Ğ»Ğ¸Ñ‡Ğ½': 2, 'Ñ…Ğ¾Ñ€Ğ¾Ñˆ': 2, 'Ğ¿Ñ€ĞµĞºÑ€Ğ°ÑĞ½': 2, 'Ğ±Ñ‹ÑÑ‚Ñ€': 1, 'ÑƒĞ´Ğ¾Ğ±Ğ½': 1, 'Ğ¿Ğ¾Ğ½ÑÑ‚Ğ½': 1,
        'Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´': 2, 'Ğ´Ğ¾Ğ²Ğ¾Ğ»': 2, 'ÑĞ¿Ğ°ÑĞ¸Ğ±': 2, 'Ñ€Ğ°Ğ´': 2, 'Ğ»ĞµĞ³Ğº': 1, 'Ğ¿Ñ€Ğ¸ÑÑ‚Ğ½': 1,
        'ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²ĞµĞ½': 2, 'Ğ¿Ñ€Ğ¾Ñ„ĞµÑÑĞ¸Ğ¾Ğ½Ğ°Ğ»': 2, 'Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²': 1, 'Ñ‡ĞµÑ‚Ğº': 1, 'Ğ¿Ñ€Ğ¾Ğ·Ñ€Ğ°Ñ‡Ğ½': 1,
        'Ğ²Ñ‹Ğ³Ğ¾Ğ´Ğ½': 2, 'Ğ½Ğ°Ğ´ĞµĞ¶Ğ½': 2, 'Ğ»ÑƒÑ‡Ñˆ': 2, 'ÑÑƒĞ¿ĞµÑ€': 2, 'Ğ·Ğ°Ğ¼ĞµÑ‡Ğ°Ñ‚ĞµĞ»ÑŒĞ½': 2, 'Ğ²Ğ¿ĞµÑ‡Ğ°Ñ‚Ğ»': 2,
        'ÑƒĞ´Ğ¾Ğ²Ğ»ĞµÑ‚Ğ²Ğ¾Ñ€ĞµĞ½': 2, 'Ğ¿Ğ¾Ğ½Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ¾ÑÑŒ': 2, 'Ğ½Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑÑ': 2, 'Ğ²Ğ¾Ğ²Ñ€ĞµĞ¼Ñ': 1, 'ÑĞ²Ğ¾ĞµĞ²Ñ€ĞµĞ¼ĞµĞ½': 1,
        'Ğ³Ğ»Ğ°Ğ´ĞºĞ¾': 1, 'ÑÑ„Ñ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½': 1, 'Ğ±ĞµĞ· Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼': 2, 'Ğ½Ğµ Ğ¿Ğ»Ğ¾Ñ…Ğ¾': 1, 'Ğ±ĞµĞ· Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº': 2,
        'Ğ½Ğµ Ğ¼ĞµĞ´Ğ»ĞµĞ½': 1, 'Ğ½Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ°ĞµÑ‚': 2
    },
    'negative': {
        'Ğ¿Ğ»Ğ¾Ñ…': -2, 'ÑƒĞ¶Ğ°ÑĞ½': -3, 'Ğ¼ĞµĞ´Ğ»ĞµĞ½': -2, 'Ğ½ĞµÑƒĞ´Ğ¾Ğ±Ğ½': -2, 'ÑĞ»Ğ¾Ğ¶Ğ½': -2, 'Ğ½Ğµ Ğ½Ñ€Ğ°Ğ²Ğ¸Ñ‚ÑÑ': -3,
        'Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼': -2, 'Ğ¾ÑˆĞ¸Ğ±Ğº': -2, 'Ğ³Ğ»ÑĞº': -2, 'Ğ·Ğ°Ğ²Ğ¸ÑĞ°': -2, 'Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚': -3, 'Ğ¾Ñ‚ĞºĞ°Ğ·': -2,
        'Ğ¾Ğ±Ğ¼Ğ°Ğ½': -3, 'Ğ´Ğ¾Ñ€Ğ¾Ğ³': -2, 'ĞºĞ¾Ğ¼Ğ¸ÑÑ': -1, 'Ğ´Ğ¾Ğ»Ğ³': -2, 'Ğ½ĞµÑÑĞ½': -1, 'Ğ½ĞµĞ¿Ğ¾Ğ»Ğ°Ğ´Ğº': -2,
        'Ğ½ĞµĞ´Ğ¾Ğ²Ğ¾Ğ»': -2, 'Ñ€Ğ°Ğ·Ğ¾Ñ‡Ğ°Ñ€Ğ¾Ğ²Ğ°Ğ½': -3, 'ĞºĞ¾ÑˆĞ¼Ğ°Ñ€': -3, 'Ğ·Ğ°Ğ²Ğ¸ÑĞ°ĞµÑ‚': -2, 'Ğ²Ğ¸ÑĞ½ĞµÑ‚': -2, 'Ñ‚ÑƒĞ¿Ğ¸Ñ‚': -2,
        'Ğ»Ğ°Ğ³Ğ°ĞµÑ‚': -2, 'Ğ¼Ğ°Ğ»ĞµĞ½ÑŒĞº': -1
    },
    'neutral': {
        'Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½': 0, 'Ğ¾Ğ±Ñ‹Ñ‡Ğ½': 0, 'ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½': 0, 'Ğ¿Ñ€Ğ¸ĞµĞ¼Ğ»ĞµĞ¼': 0, 'Ğ¸Ğ·Ğ¼ĞµĞ½ĞµĞ½': 0,
        'Ğ¾Ğ¶Ğ¸Ğ´Ğ°': 0, 'ÑÑ€ĞµĞ´Ğ½': 0, 'Ñ€Ğ°Ğ±Ğ¾Ñ‚': 0, 'Ğ½ĞµĞ¹Ñ‚Ñ€Ğ°Ğ»ÑŒĞ½': 0
    }
}

# Ğ£ÑĞ¸Ğ»Ğ¸Ğ²Ğ°ÑÑ‰Ğ¸Ğµ ÑĞ»Ğ¾Ğ²Ğ° Ğ¸ Ğ¾Ñ‚Ñ€Ğ¸Ñ†Ğ°Ğ½Ğ¸Ñ
INTENSIFIERS = {
    'Ğ¾Ñ‡ĞµĞ½ÑŒ': 1.5, 'ĞºÑ€Ğ°Ğ¹Ğ½Ğµ': 2, 'ÑĞ¾Ğ²ÑĞµĞ¼': 1.5, 'Ğ°Ğ±ÑĞ¾Ğ»ÑÑ‚Ğ½Ğ¾': 2, 'Ğ¿Ğ¾Ğ»Ğ½Ğ¾ÑÑ‚ÑŒÑ': 1.5, 'ÑĞ¸Ğ»ÑŒĞ½Ğ¾': 1.5,
    'Ñ‡Ñ€ĞµĞ·Ğ²Ñ‹Ñ‡Ğ°Ğ¹Ğ½Ğ¾': 2, 'Ğ½ĞµĞ²ĞµÑ€Ğ¾ÑÑ‚Ğ½Ğ¾': 2, 'ÑƒĞ´Ğ¸Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾': 1.5, 'Ğ½ĞµĞ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾': 1.5, 'Ğ²ĞµÑÑŒĞ¼Ğ°': 1.5,
    'ÑĞ»Ğ¸ÑˆĞºĞ¾Ğ¼': 2, 'Ñ‡Ğ°ÑÑ‚Ğ¾': 1
}

NEGATION_WORDS = {
    'Ğ½Ğµ', 'Ğ½ĞµÑ‚', 'Ğ½Ğ¸', 'Ğ±ĞµĞ·', 'Ğ½ĞµĞ»ÑŒĞ·Ñ', 'Ğ½ĞµĞ²Ğ¾Ğ·Ğ¼Ğ¾Ğ¶Ğ½Ğ¾', 'Ğ½Ğ¸ĞºĞ°Ğº', 'Ğ½Ğ¸Ñ‡ÑƒÑ‚ÑŒ'
}

# Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ´Ğ»Ñ Ñ‚ĞµĞ¼
PRODUCT_CATEGORIES_TOPICS = {
    'ĞĞ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ': {
        'keywords': ['Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½', 'Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸', 'ĞºĞ»Ğ¸ĞµĞ½Ñ‚', 'Ğ¾Ñ‡ĞµÑ€ĞµĞ´', 'Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»', 'Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€', 'ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ğ½Ñ‚', 'Ğ¿Ñ€Ğ¸ĞµĞ¼', 'ĞºĞ°ÑÑĞ°', 'Ğ¾Ğ¿ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€', 'Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ'],
        'phrases': ['Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ² Ğ±Ğ°Ğ½ĞºĞµ', 'Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ½ĞºĞ°', 'Ğ¿ĞµÑ€ÑĞ¾Ğ½Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€', 'ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ñ Ğ² Ğ±Ğ°Ğ½ĞºĞµ', 'Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğ¸']
    },
    'ĞœĞ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ': {
        'keywords': ['Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒ', 'Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½', 'Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½', 'Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚', 'ÑĞ¼Ñ', 'Ğ³Ğ»ÑĞº', 'Ğ·Ğ°Ğ²Ğ¸ÑĞ°', 'Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚', 'Ñ‚Ğ¾Ñ€Ğ¼Ğ¾Ğ·', 'Ğ²Ğ¸ÑĞ½ĞµÑ‚', 'Ñ‚ÑƒĞ¿Ğ¸Ñ‚', 'Ğ»Ğ°Ğ³Ğ°ĞµÑ‚'],
        'phrases': ['Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğº', 'Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ', 'Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ Ğ±Ğ°Ğ½Ğº', 'ÑĞ¼Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ²Ğ¸ÑĞ»Ğ¾']
    },
    'ĞšÑ€ĞµĞ´Ğ¸Ñ‚Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ°': {
        'keywords': ['ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ½', 'Ğ»Ğ¸Ğ¼Ğ¸Ñ‚', 'Ğ¾Ğ´Ğ¾Ğ±Ñ€ĞµĞ½', 'Ğ¿Ğ¾Ğ³Ğ°ÑˆĞµĞ½', 'Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚', 'Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶', 'Ğ·Ğ°ÑĞ²Ğº'],
        'phrases': ['ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ°', 'Ğ¾Ğ´Ğ¾Ğ±Ñ€ĞµĞ½Ğ¸Ğµ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°', 'Ğ¿Ğ¾Ğ³Ğ°ÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°', 'ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ½Ñ‹Ğ¹ Ğ»Ğ¸Ğ¼Ğ¸Ñ‚', 'Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ‚ÑŒ ĞºĞ°Ñ€Ñ‚Ñƒ']
    }
}

# Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€ÑŒ Ğ´Ğ»Ñ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ²
PRODUCT_CATEGORIES_MAIN = {
    'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸': {
        'subcategories': {
            'Ğ’ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ»ÑÑ‚Ğ½Ñ‹Ñ… ÑÑ‡ĞµÑ‚Ğ¾Ğ²': {'keywords': ['Ğ²Ğ°Ğ»ÑÑ‚', 'Ñ€ÑƒĞ±Ğ»', 'Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€', 'ĞµĞ²Ñ€Ğ¾', 'Ğ²Ğ°Ğ»ÑÑ‚Ğ½', 'ÑÑ‡ĞµÑ‚', 'ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†', 'Ğ¾Ğ±Ğ¼ĞµĞ½'], 'phrases': ['Ğ²Ğ°Ğ»ÑÑ‚Ğ½Ñ‹Ğ¹ ÑÑ‡ĞµÑ‚', 'Ğ¾Ğ±Ğ¼ĞµĞ½ Ğ²Ğ°Ğ»ÑÑ‚Ñ‹', 'ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ²Ğ°Ğ»ÑÑ‚Ñ‹']},
            'Ğ”ĞµĞ±ĞµÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹': {'keywords': ['Ğ´ĞµĞ±ĞµÑ‚', 'ÑĞ½ÑÑ‚Ğ¸Ğµ', 'Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ', 'ĞºÑÑˆĞ±ÑĞº'], 'phrases': ['Ğ´ĞµĞ±ĞµÑ‚Ğ¾Ğ²Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ°', 'Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹', 'ÑĞ½ÑÑ‚Ğ¸Ğµ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ…']},
            'ĞœĞ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğº': {'keywords': ['Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒ', 'Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½', 'Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½', 'Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚', 'ÑĞ¼Ñ'], 'phrases': ['Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğº', 'Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ', 'Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ Ğ±Ğ°Ğ½Ğº']},
            'ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‹': {'keywords': ['Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´', 'ÑÑ€ĞµĞ´ÑÑ‚Ğ²', 'Ğ´ĞµĞ½ÑŒĞ³', 'Ğ¿ĞµÑ€ĞµÑ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ'], 'phrases': ['Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ´ĞµĞ½ĞµĞ³', 'Ğ¿ĞµÑ€ĞµÑ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ ÑÑ€ĞµĞ´ÑÑ‚Ğ²']},
            'Ğ—Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹': {'keywords': ['Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚', 'Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ½', 'Ğ²Ñ‹Ğ¿Ğ»Ğ°Ñ‚Ğ°'], 'phrases': ['Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ°', 'Ğ²Ñ‹Ğ¿Ğ»Ğ°Ñ‚Ğ° Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ñ‹']}
        },
        'keywords': ['Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´', 'Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚', 'Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒ', 'Ğ±Ğ°Ğ½Ğº', 'Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶', 'Ğ²Ğ°Ğ»ÑÑ‚', 'Ñ€ÑƒĞ±Ğ»', 'Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€', 'ĞµĞ²Ñ€Ğ¾', 'ÑĞ½ÑÑ‚', 'Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½', 'Ğ±Ğ°Ğ½ĞºĞ¾Ğ¼Ğ°Ñ‚', 'Ğ¾Ğ¿Ğ»Ğ°Ñ‚', 'ĞºĞ²Ğ¸Ñ‚Ğ°Ğ½Ñ†', 'ĞºĞ¾Ğ¼Ğ¸ÑÑ', 'Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ²Ğ°Ğ½', 'Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸', 'ĞºĞ»Ğ¸ĞµĞ½Ñ‚', 'Ğ¾Ñ‡ĞµÑ€ĞµĞ´', 'Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½', 'Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½', 'Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚', 'ÑĞ¼Ñ', 'ÑƒĞ²ĞµĞ´Ğ¾Ğ¼Ğ»ĞµĞ½', 'ÑÑ€ĞµĞ´ÑÑ‚Ğ²', 'Ğ´ĞµĞ½ÑŒĞ³', 'Ğ½Ğ°Ğ»Ğ¸Ñ‡', 'Ğ±ĞµĞ·Ğ½Ğ°Ğ»', 'ÑĞ±ĞµÑ€ĞºĞ½Ğ¸Ğ¶Ğº', 'Ğ²Ñ‹Ğ¿Ğ¸Ñ', 'Ğ±Ğ°Ğ»Ğ°Ğ½Ñ', 'Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº'],
        'phrases': ['Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ½Ğ°Ñ ĞºĞ°Ñ€Ñ‚Ğ°', 'Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğº', 'Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´ Ğ´ĞµĞ½ĞµĞ³', 'Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ ÑÑ‡ĞµÑ‚', 'Ğ²Ğ°Ğ»ÑÑ‚Ğ½Ñ‹Ğ¹ ÑÑ‡ĞµÑ‚', 'Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ ÑÑ‡ĞµÑ‚', 'Ñ€Ğ°ÑÑ‡ĞµÑ‚Ğ½Ñ‹Ğ¹ ÑÑ‡ĞµÑ‚', 'ÑĞ½ÑÑ‚ÑŒ Ğ´ĞµĞ½ÑŒĞ³Ğ¸', 'Ğ¿Ğ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ´ĞµĞ½ÑŒĞ³Ğ¸', 'Ğ¾Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ñ‚ÑŒ ÑƒÑĞ»ÑƒĞ³Ğ¸', 'ĞºĞ¾Ğ¼Ğ¸ÑÑĞ¸Ñ Ğ·Ğ° Ğ¿ĞµÑ€ĞµĞ²Ğ¾Ğ´', 'Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹', 'Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ±Ğ°Ğ½ĞºĞ°', 'Ğ¾Ñ‡ĞµÑ€ĞµĞ´ÑŒ Ğ² Ğ±Ğ°Ğ½ĞºĞµ', 'Ğ¸Ğ½Ñ‚ĞµÑ€Ğ½ĞµÑ‚ Ğ±Ğ°Ğ½Ğº', 'Ğ¼Ğ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¿Ñ€Ğ¸Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ğµ', 'ÑĞ¼Ñ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ', 'Ğ±ĞµĞ·Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ñ€Ğ°ÑÑ‡ĞµÑ‚', 'Ğ²Ñ‹Ğ¿Ğ¸ÑĞºĞ° Ğ¿Ğ¾ ÑÑ‡ĞµÑ‚Ñƒ', 'Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ğº Ğ½Ğ° ÑÑ‡ĞµÑ‚Ğµ']
    },
    'Ğ¡Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ': {
        'keywords': ['Ğ²ĞºĞ»Ğ°Ğ´', 'ÑĞ±ĞµÑ€ĞµĞ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½', 'Ğ½Ğ°ĞºĞ¾Ğ¿Ğ¸Ñ‚', 'ÑĞ±ĞµÑ€ĞµĞ¶ĞµĞ½', 'Ğ¼ĞµÑ‚Ğ°Ğ»Ğ»', 'ÑÑ‡ĞµÑ‚', 'ÑÑ€Ğ¾Ñ‡Ğ½', 'Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚', 'Ğ½Ğ°ĞºĞ¾Ğ¿', 'ÑĞ±ĞµÑ€ĞµĞ³', 'Ğ´ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚', 'ÑÑ‚Ğ°Ğ²Ğº', 'Ğ´Ğ¾Ñ…Ğ¾Ğ´Ğ½Ğ¾ÑÑ‚', 'ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†', 'Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½', 'ÑĞ½ÑÑ‚', 'Ğ¿Ñ€Ğ¾Ğ»Ğ¾Ğ½Ğ³Ğ°Ñ†', 'Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ñ‹', 'Ğ½Ğ°Ñ‡Ğ¸ÑĞ»ĞµĞ½', 'Ğ·Ğ°Ğ±Ñ€Ğ°Ñ‚', 'Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½', 'Ğ·Ğ¾Ğ»Ğ¾Ñ‚', 'ÑĞµÑ€ĞµĞ±Ñ€', 'Ğ¿Ğ»Ğ°Ñ‚Ğ¸Ğ½', 'Ğ¿Ğ°Ğ»Ğ»Ğ°Ğ´', 'ÑĞ»Ğ¸Ñ‚Ğº', 'ÑĞ±ĞµÑ€ĞµĞ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½', 'ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚', 'Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½', 'Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚', 'Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚'],
        'phrases': ['ÑÑ€Ğ¾Ñ‡Ğ½Ñ‹Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´', 'ÑĞ±ĞµÑ€ĞµĞ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‡ĞµÑ‚', 'Ğ¼ĞµÑ‚Ğ°Ğ»Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹ ÑÑ‡ĞµÑ‚', 'Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ²ĞºĞ»Ğ°Ğ´', 'Ğ½Ğ°ĞºĞ¾Ğ¿Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑÑ‡ĞµÑ‚', 'Ğ¾Ğ±ĞµĞ·Ğ»Ğ¸Ñ‡ĞµĞ½Ğ½Ñ‹Ğ¹ Ğ¼ĞµÑ‚Ğ°Ğ»Ğ»Ğ¸Ñ‡ĞµÑĞºĞ¸Ğ¹', 'Ğ±Ğ°Ğ½ĞºĞ¾Ğ²ÑĞºĞ¸Ğ¹ Ğ²ĞºĞ»Ğ°Ğ´', 'Ğ´ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚Ğ½Ñ‹Ğ¹ ÑÑ‡ĞµÑ‚', 'Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ½Ğ°Ñ ÑÑ‚Ğ°Ğ²ĞºĞ°', 'Ğ´Ğ¾Ñ…Ğ¾Ğ´ Ğ¿Ğ¾ Ğ²ĞºĞ»Ğ°Ğ´Ñƒ', 'ĞºĞ°Ğ¿Ğ¸Ñ‚Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ¾Ğ²', 'Ğ¿Ğ¾Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ñ‚ÑŒ Ğ²ĞºĞ»Ğ°Ğ´', 'ÑĞ½ÑÑ‚ÑŒ ÑĞ¾ Ğ²ĞºĞ»Ğ°Ğ´Ğ°', 'Ğ¿Ñ€Ğ¾Ğ»Ğ¾Ğ½Ğ³Ğ°Ñ†Ğ¸Ñ Ğ²ĞºĞ»Ğ°Ğ´Ğ°', 'Ğ½Ğ°Ñ‡Ğ¸ÑĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ¾Ğ²', 'Ğ·Ğ°Ğ±Ñ€Ğ°Ñ‚ÑŒ Ğ²ĞºĞ»Ğ°Ğ´', 'Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚ÑŒ Ğ´ĞµĞ¿Ğ¾Ğ·Ğ¸Ñ‚', 'Ğ·Ğ¾Ğ»Ğ¾Ñ‚Ğ¾Ğ¹ ÑĞ»Ğ¸Ñ‚Ğ¾Ğº', 'ÑĞ±ĞµÑ€ĞµĞ³Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ÑĞµÑ€Ñ‚Ğ¸Ñ„Ğ¸ĞºĞ°Ñ‚', 'Ğ½Ğ°ĞºĞ¾Ğ¿Ğ¸Ñ‚ÑŒ Ğ´ĞµĞ½ÑŒĞ³Ğ¸']
    },
    'ĞšÑ€ĞµĞ´Ğ¸Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ': {
        'keywords': ['ĞºÑ€ĞµĞ´Ğ¸Ñ‚', 'Ğ·Ğ°Ğ¹Ğ¼', 'Ğ¸Ğ¿Ğ¾Ñ‚ĞµĞº', 'Ğ°Ğ²Ñ‚Ğ¾ĞºÑ€ĞµĞ´Ğ¸Ñ‚', 'ÑÑÑƒĞ´', 'Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞº', 'Ñ€ĞµÑ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸', 'Ğ´Ğ¾Ğ»Ğ³', 'Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚', 'Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶', 'Ğ¿Ğ¾Ğ³Ğ°ÑˆĞµĞ½Ğ¸', 'ÑÑ‚Ğ°Ğ²Ğº', 'ÑÑ€Ğ¾Ğº', 'Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚', 'Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½', 'Ğ¾Ñ‚ĞºĞ°Ğ·', 'Ğ¾Ğ´Ğ¾Ğ±Ñ€ĞµĞ½', 'Ğ¿ĞµÑ€ĞµĞ¿Ğ»Ğ°Ñ‚Ğ°', 'Ğ·Ğ°Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½', 'Ğ¿Ñ€Ğ¾ÑÑ€Ğ¾Ñ‡Ğº'],
        'phrases': ['Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±Ğ¸Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ ĞºÑ€ĞµĞ´Ğ¸Ñ‚', 'Ğ¸Ğ¿Ğ¾Ñ‚ĞµÑ‡Ğ½Ñ‹Ğ¹ ĞºÑ€ĞµĞ´Ğ¸Ñ‚', 'Ğ°Ğ²Ñ‚Ğ¾ĞºÑ€ĞµĞ´Ğ¸Ñ‚', 'Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ‚ÑŒ ĞºÑ€ĞµĞ´Ğ¸Ñ‚', 'Ñ€ĞµÑ„Ğ¸Ğ½Ğ°Ğ½ÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°', 'Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚ Ğ¿Ğ¾ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ñƒ', 'Ğ¿Ğ¾Ğ³Ğ°ÑˆĞµĞ½Ğ¸Ğµ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°', 'ÑÑ‚Ğ°Ğ²ĞºĞ° Ğ¿Ğ¾ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ñƒ', 'Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ´Ğ»Ñ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğ°', 'Ğ¾Ñ‚ĞºĞ°Ğ· Ğ² ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ğµ', 'Ğ¿ĞµÑ€ĞµĞ¿Ğ»Ğ°Ñ‚Ğ° Ğ¿Ğ¾ ĞºÑ€ĞµĞ´Ğ¸Ñ‚Ñƒ']
    },
    'Ğ˜Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ğ¸Ğ¸': {
        'keywords': ['Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†', 'Ğ±Ñ€Ğ¾ĞºĞµÑ€', 'Ğ°ĞºÑ†Ğ¸', 'Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†', 'Ñ„Ğ¾Ğ½Ğ´Ñ‹', 'Ğ¿Ğ¾Ñ€Ñ‚Ñ„ĞµĞ»ÑŒ', 'Ğ´Ğ¾Ñ…Ğ¾Ğ´', 'Ñ€Ğ¸ÑĞº', 'Ğ´Ğ¸Ğ²Ğ¸Ğ´ĞµĞ½Ğ´', 'Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²', 'Ğ±Ğ¸Ñ€Ğ¶', 'ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½', 'Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ', 'ÑƒĞ±Ñ‹Ñ‚Ğ¾Ğº', 'Ğ¸Ğ½Ğ´ĞµĞºÑ'],
        'phrases': ['Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾Ñ€Ñ‚Ñ„ĞµĞ»ÑŒ', 'Ğ¿Ğ¾ĞºÑƒĞ¿ĞºĞ° Ğ°ĞºÑ†Ğ¸Ğ¹', 'Ğ¾Ğ±Ğ»Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¸ Ğ±Ğ°Ğ½ĞºĞ°', 'Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ„Ğ¾Ğ½Ğ´Ñ‹', 'ÑƒĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ°Ğ¼Ğ¸', 'Ğ´Ğ¸Ğ²Ğ¸Ğ´ĞµĞ½Ğ´Ñ‹ Ğ¿Ğ¾ Ğ°ĞºÑ†Ğ¸ÑĞ¼', 'Ñ‚Ğ¾Ñ€Ğ³Ğ¾Ğ²Ğ»Ñ Ğ½Ğ° Ğ±Ğ¸Ñ€Ğ¶Ğµ', 'Ğ¸Ğ½Ğ´ĞµĞºÑĞ½Ñ‹Ğ¹ Ñ„Ğ¾Ğ½Ğ´']
    },
    'Ğ¡Ñ‚Ñ€Ğ°Ñ…Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ°': {
        'keywords': ['ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²Ğº', 'Ğ¿Ğ¾Ğ»Ğ¸Ñ', 'Ğ²Ñ‹Ğ¿Ğ»Ğ°Ñ‚', 'ÑƒÑ‰ĞµÑ€Ğ±', 'ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²Ğ°Ğ½', 'Ñ€Ğ¸ÑĞº', 'Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½', 'Ğ¿Ñ€ĞµÑ‚ĞµĞ½Ğ·Ğ¸', 'Ğ²Ğ¾Ğ·Ğ¼ĞµÑ‰ĞµĞ½', 'Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚', 'ÑÑ€Ğ¾Ğº', 'ÑƒÑĞ»ÑƒĞ³', 'ÑƒÑ‰ĞµÑ€Ğ±'],
        'phrases': ['ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²Ğ¾Ğ¹ Ğ¿Ğ¾Ğ»Ğ¸Ñ', 'Ğ¾Ñ„Ğ¾Ñ€Ğ¼Ğ»ĞµĞ½Ğ¸Ğµ ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²ĞºĞ¸', 'Ğ²Ñ‹Ğ¿Ğ»Ğ°Ñ‚Ğ° Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²ĞºĞµ', 'Ğ²Ğ¾Ğ·Ğ¼ĞµÑ‰ĞµĞ½Ğ¸Ğµ ÑƒÑ‰ĞµÑ€Ğ±Ğ°', 'ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¶Ğ¸Ğ·Ğ½Ğ¸', 'Ğ¿Ñ€ĞµÑ‚ĞµĞ½Ğ·Ğ¸Ñ Ğ¿Ğ¾ ÑÑ‚Ñ€Ğ°Ñ…Ğ¾Ğ²ĞºĞµ']
    },
    'ĞŸÑ€ĞµĞ¼Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»ÑƒĞ³Ğ¸': {
        'keywords': ['Ğ¿Ñ€ĞµĞ¼Ğ¸ÑƒĞ¼', 'Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸', 'ÑĞ»Ğ¸Ñ‚', 'VIP', 'Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½', 'ÑƒÑĞ»ÑƒĞ³', 'Ğ´Ğ¾ÑÑ‚ÑƒĞ¿', 'ÑĞºĞ¸Ğ´Ğº', 'Ğ±Ğ¾Ğ½ÑƒÑ', 'ÑÑ‚Ğ°Ñ‚ÑƒÑ', 'ĞºĞ°Ñ€Ñ‚Ğ°', 'Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½'],
        'phrases': ['Ğ¿Ñ€ĞµĞ¼Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğµ Ğ¾Ğ±ÑĞ»ÑƒĞ¶Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ', 'Ğ¿Ñ€Ğ¸Ğ²Ğ¸Ğ»ĞµĞ³Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ ĞºĞ»Ğ¸ĞµĞ½Ñ‚', 'VIP-ĞºĞ°Ñ€Ñ‚Ğ°', 'ÑĞ»Ğ¸Ñ‚Ğ½Ñ‹Ğ¹ ÑÑ‚Ğ°Ñ‚ÑƒÑ', 'Ğ±Ğ¾Ğ½ÑƒÑĞ½Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°']
    }
}

# Ğ¤ÑƒĞ½ĞºÑ†Ğ¸Ğ¸ ĞºĞ»Ğ°ÑÑĞ¸Ñ„Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸
def classify_sentiment(text):
    text = text.lower()
    sentiment_score = 0
    words = re.findall(r'\w+', text)
    
    for i, word in enumerate(words):
        base_score = 0
        for sentiment, keywords in SENTIMENT_LEXICON.items():
            for key, score in keywords.items():
                if key in word:
                    base_score = score
                    break
            if base_score != 0:
                break
        
        if base_score != 0:
            if i > 0 and words[i-1] in INTENSIFIERS:
                base_score *= INTENSIFIERS[words[i-1]]
            if i > 0 and words[i-1] in NEGATION_WORDS:
                base_score = -base_score
            
            sentiment_score += base_score
    
    if sentiment_score > 1:
        return 'positive'
    elif sentiment_score < -1:
        return 'negative'
    return 'neutral'

def classify_topics(text):
    text = text.lower()
    categories = set()
    words = set(re.findall(r'\w+', text))
    
    for category, data in PRODUCT_CATEGORIES_TOPICS.items():
        keywords = set(data['keywords'])
        phrases = set(data['phrases'])
        if any(word in keywords for word in words) or any(phrase in text for phrase in phrases):
            categories.add(category)
    
    return list(categories) if categories else ['Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ']

def classify_product_category(text, topics):
    text = text.lower()
    categories = set()
    words = set(re.findall(r'\w+', text))
    
    for category, data in PRODUCT_CATEGORIES_MAIN.items():
        keywords = set(data['keywords'])
        phrases = set(data['phrases'])
        matched = False
        if any(word in keywords for word in words) or any(phrase in text for phrase in phrases):
            matched = True
        if matched and 'subcategories' in data:
            for subcategory, subdata in data['subcategories'].items():
                sub_keywords = set(subdata['keywords'])
                sub_phrases = set(subdata['phrases'])
                sub_matched = False
                if any(word in sub_keywords for word in words) or any(phrase in text for phrase in sub_phrases):
                    sub_matched = True
                if sub_matched:
                    categories.add(f"{category} - {subcategory}")
        else:
            categories.add(category)

    if len(categories) > 1 and 'Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ' in categories:
        categories.remove('Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ')
    return list(categories) if categories else ['Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ']

def process_review(review):
    text = review.get('text', '')
    id = review.get('id', 0)
    
    parts = re.split(r'\bĞ½Ğ¾\b', text, flags=re.IGNORECASE)
    parts = [p.strip() for p in parts if p.strip()]
    
    topics = []
    sentiments = []
    
    if parts:
        for part in parts:
            part_topics = classify_topics(part)
            sentiment = classify_sentiment(part)
            for topic in part_topics:
                if topic not in topics:
                    topics.append(topic)
                    sentiments.append(sentiment)
    else:
        topics = classify_topics(text)
        sentiment = classify_sentiment(text)
        sentiments = [sentiment] * len(topics)
    
    if len(topics) > 1 and 'Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ' in topics:
        idx = topics.index('Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ')
        topics.pop(idx)
        sentiments.pop(idx)
    
    current_date = datetime.now().strftime('%d.%m.%Y')
    rating_map = {'negative': 1, 'neutral': 3, 'positive': 5}
    source = 'gold'
    author = review.get('author', 'ĞšĞ»Ğ¸ĞµĞ½Ñ‚ Ğ±Ğ°Ğ½ĞºĞ°')
    title = ' '.join(re.findall(r'\w+', text)[:5]) if text else 'Ğ‘ĞµĞ· Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°'
    product_categories = classify_product_category(text, topics)
    
    try:
        df = pd.read_csv('gazprombank_reviews_classified.csv', sep=';', encoding='utf-8-sig')
    except FileNotFoundError:
        df = pd.DataFrame(columns=['text', 'topics', 'sentiments', 'date', 'rating', 'source', 'id', 'author', 'title', 'product_category'])
    except Exception as e:
        logging.error(f"Error reading CSV file: {str(e)}")
        return {'id': id, 'topics': topics, 'sentiments': sentiments, 'error': f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ñ‡Ñ‚ĞµĞ½Ğ¸Ğ¸ Ñ„Ğ°Ğ¹Ğ»Ğ°: {str(e)}"}
    
    topics_str = ', '.join(topics)
    sentiments_str = ', '.join(sentiments)
    product_category_str = ', '.join(product_categories)
    
    avg_rating = sum(rating_map.get(s, 3) for s in sentiments) / len(sentiments) if sentiments else 3
    
    try:
        new_review = pd.DataFrame({
            'text': [text],
            'topics': [topics_str],
            'sentiments': [sentiments_str],
            'date': [current_date],
            'rating': [round(avg_rating)],
            'source': [source],
            'id': [id],
            'author': [author],
            'title': [title],
            'product_category': [product_category_str]
        })
        df = pd.concat([df, new_review], ignore_index=True)
        df.to_csv('gazprombank_reviews_classified.csv', index=False, sep=';', encoding='utf-8-sig')
    except Exception as e:
        logging.error(f"Error writing to CSV file: {str(e)}")
        return {'id': id, 'topics': topics, 'sentiments': sentiments, 'error': f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ Ğ·Ğ°Ğ¿Ğ¸ÑĞ¸ Ğ² Ñ„Ğ°Ğ¹Ğ»: {str(e)}"}
    
    return {'id': id, 'topics': topics, 'sentiments': sentiments, 'product_category': product_categories}

@st.cache_data
def load_data(uploaded_file, file_type):
    if uploaded_file is not None:
        if file_type == 'csv':
            df = pd.read_csv(uploaded_file, sep=';', encoding='utf-8-sig', on_bad_lines='skip')
        else:  # json
            data = json.load(uploaded_file)
            if 'data' in data and isinstance(data['data'], list):
                predictions = []
                for review in data['data']:
                    result = process_review(review)
                    if 'error' not in result:
                        predictions.append(result)
                # Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ DataFrame Ğ±ĞµĞ· Ñ€Ğ°Ğ·Ğ±Ğ¸ĞµĞ½Ğ¸Ñ Ğ½Ğ° ÑÑ‚Ñ€Ğ¾ĞºĞ¸
                rows = []
                for pred, orig in zip(predictions, data['data']):
                    row = {
                        'id': pred['id'],
                        'text': orig.get('text', ''),
                        'topics': ', '.join(pred['topics']),
                        'sentiments': ', '.join(pred['sentiments']),
                        'product_category': ', '.join(pred['product_category']),
                        'date': orig.get('date', datetime.now().strftime('%d.%m.%Y')),
                        'rating': orig.get('rating', 3),
                        'author': orig.get('author', 'ĞšĞ»Ğ¸ĞµĞ½Ñ‚ Ğ±Ğ°Ğ½ĞºĞ°'),
                        'source': orig.get('source', 'gold'),
                        'title': ' '.join(re.findall(r'\w+', orig.get('text', ''))[:5]) if orig.get('text', '') else 'Ğ‘ĞµĞ· Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°'
                    }
                    rows.append(row)
                df = pd.DataFrame(rows)
            else:
                st.error("ĞĞµĞ²ĞµÑ€Ğ½Ñ‹Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ JSON. ĞĞ¶Ğ¸Ğ´Ğ°ĞµÑ‚ÑÑ {'data': [{'id': 1, 'text': '...'}]}")
                return pd.DataFrame()

        if not df.empty:
            required_cols = ['text']
            if not all(col in df.columns for col in required_cols):
                missing_cols = [col for col in required_cols if col not in df.columns]
                st.error(f"ĞÑ‚ÑÑƒÑ‚ÑÑ‚Ğ²ÑƒÑÑ‚ ĞºĞ¾Ğ»Ğ¾Ğ½ĞºĞ¸: {missing_cols}. ĞŸÑ€Ğ¾Ğ²ĞµÑ€ÑŒÑ‚Ğµ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ.")
                return pd.DataFrame()

            if 'product_category' not in df.columns:
                df['product_category'] = df['text'].apply(lambda x: ', '.join(classify_product_category(x, classify_topics(x))))
            if 'sentiments' not in df.columns:
                df['sentiments'] = df['text'].apply(classify_sentiment)
            if 'topics' not in df.columns:
                df['topics'] = df['text'].apply(lambda x: ', '.join(classify_topics(x)))

            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'], format='%d.%m.%Y', errors='coerce')
            if 'id' not in df.columns:
                df['id'] = df.index + 1

            st.info(f"Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ¾ {len(df)} ÑÑ‚Ñ€Ğ¾Ğº. Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ‹: {sorted(df['product_category'].unique())}")
            if 'date' in df.columns:
                st.info(f"Ğ”Ğ¸Ğ°Ğ¿Ğ°Ğ·Ğ¾Ğ½ Ğ´Ğ°Ñ‚: {df['date'].min().strftime('%d.%m.%Y')} - {df['date'].max().strftime('%d.%m.%Y')}")

            return df
    return pd.DataFrame()

if uploaded_csv or uploaded_json:
    df = load_data(uploaded_csv if uploaded_csv else uploaded_json, 'csv' if uploaded_csv else 'json')
else:
    df = pd.DataFrame()
    st.sidebar.warning("Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ CSV Ğ¸Ğ»Ğ¸ JSON Ñ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ğ¼Ğ¸ Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°")

if not df.empty:
    st.sidebar.header("Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ñ‹")
    min_date = df['date'].min().date() if 'date' in df and pd.notna(df['date'].min()) else datetime(2024, 1, 1).date()
    max_date = df['date'].max().date() if 'date' in df and pd.notna(df['date'].max()) else datetime(2025, 5, 31).date()
    start_date = st.sidebar.date_input("ĞĞ°Ñ‡Ğ°Ğ»ÑŒĞ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°", min_date, min_value=min_date, max_value=max_date)
    end_date = st.sidebar.date_input("ĞšĞ¾Ğ½ĞµÑ‡Ğ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°", max_date, min_value=min_date, max_value=max_date)

    source_options = ['Ğ’ÑĞµ'] + sorted(df['source'].dropna().unique().tolist()) if 'source' in df.columns else ['Ğ’ÑĞµ']
    source_filter = st.sidebar.multiselect("Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº", options=source_options, default=['Ğ’ÑĞµ'])

    sentiment_options = ['Ğ’ÑĞµ'] + ['positive', 'negative', 'neutral']
    sentiment_filter = st.sidebar.multiselect("Ğ¢Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ", options=sentiment_options, default=['Ğ’ÑĞµ'])

    main_product_options = ['Ğ’ÑĞµ'] + sorted([cat for cat in df['product_category'].str.split(', ').explode().unique() if ' - ' not in str(cat)])
    product_filter = st.sidebar.multiselect("ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°", options=main_product_options, default=['Ğ’ÑĞµ'])

    subcategories_filter = []
    if 'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸' in product_filter and len(product_filter) == 1:
        subcategories = sorted([cat for cat in df['product_category'].str.split(', ').explode().unique() if cat.startswith('ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸ - ')])
        subcategories_filter = st.sidebar.multiselect("ĞŸĞ¾Ğ´ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ñ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°", options=subcategories, default=subcategories)

    if 'Ğ’ÑĞµ' in product_filter and len(product_filter) > 1:
        product_filter = ['Ğ’ÑĞµ']
        subcategories_filter = []
        st.rerun()

    rating_filter = st.sidebar.slider("Ğ ĞµĞ¹Ñ‚Ğ¸Ğ½Ğ³", min_value=1, max_value=5, value=(1, 5)) if 'rating' in df.columns else (1, 5)
    keyword_filter = st.sidebar.text_input("ĞšĞ»ÑÑ‡ĞµĞ²Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ Ğ² Ñ‚ĞµĞºÑÑ‚Ğµ", "")

    # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ñ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ¾Ğ¹ Ğ½Ğ°Ğ»Ğ¸Ñ‡Ğ¸Ñ ĞºĞ¾Ğ»Ğ¾Ğ½Ğ¾Ğº
    mask = pd.Series(True, index=df.index)
    if 'date' in df.columns:
        mask &= (df['date'].dt.date >= start_date) & (df['date'].dt.date <= end_date)
    if 'rating' in df.columns:
        mask &= df['rating'].between(*rating_filter)
    if 'source' in df.columns and source_filter and 'Ğ’ÑĞµ' not in source_filter:
        mask &= df['source'].isin(source_filter)
    if 'sentiments' in df.columns and sentiment_filter and 'Ğ’ÑĞµ' not in sentiment_filter:
        mask &= df['sentiments'].str.contains('|'.join(sentiment_filter), case=False, na=False)
    if product_filter and 'Ğ’ÑĞµ' not in product_filter:
        mask &= df['product_category'].str.contains('|'.join(product_filter + subcategories_filter), case=False, na=False)
    if 'text' in df.columns and keyword_filter:
        mask &= df['text'].str.contains(keyword_filter, case=False, na=False, regex=True)

    filtered_df = df[mask].copy()

    st.info(f"ĞŸĞ¾ÑĞ»Ğµ Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğ°Ñ†Ğ¸Ğ¸: {len(filtered_df)} ÑÑ‚Ñ€Ğ¾Ğº. Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ñ‹ Ğ² Ñ„Ğ¸Ğ»ÑŒÑ‚Ñ€Ğµ: {sorted(filtered_df['product_category'].str.split(', ').explode().unique())}")

    # Ğ¡Ñ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°
    st.header("ğŸ“Š ĞĞ±Ñ‰Ğ°Ñ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ°")
    group_cols = ['id', 'text'] if 'id' in filtered_df.columns else ['text']
    total_reviews = len(filtered_df.groupby(group_cols).size()) if not filtered_df.empty else 0
    st.write(f"Ğ’ÑĞµĞ³Ğ¾ ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²: {total_reviews}")

    # Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº Ñ€Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
    st.subheader("ğŸ˜Š Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸")
    if 'sentiments' in filtered_df and not filtered_df['sentiments'].isna().all():
        sentiment_counts = filtered_df.groupby(group_cols + ['sentiments']).size().groupby('sentiments').count()
        fig_sentiment = px.pie(names=sentiment_counts.index, values=sentiment_counts.values, title="Ğ¢Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²",
                              color=sentiment_counts.index,
                              color_discrete_map={'positive': '#90EE90', 'negative': '#FF6347', 'neutral': '#D3D3D3'},
                              height=600)
        st.plotly_chart(fig_sentiment, use_container_width=True)
    else:
        st.write("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸.")

    # Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°Ğ¼ Ğ¿Ğ¾ Ğ¼ĞµÑÑÑ†Ğ°Ğ¼
    st.subheader("ğŸ“ˆ Ğ”Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ° Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°Ğ¼ Ğ¿Ğ¾ Ğ¼ĞµÑÑÑ†Ğ°Ğ¼")
    if 'date' in filtered_df and 'product_category' in filtered_df and not filtered_df.empty:
        filtered_df['month'] = filtered_df['date'].dt.to_period('M').astype(str)
        product_monthly = filtered_df.groupby(['month', 'product_category']).size().reset_index(name='count')
        if not product_monthly.empty:
            if 'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸' in product_filter and len(product_filter) == 1:
                product_monthly = product_monthly[product_monthly['product_category'].str.contains('ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸')]
            else:
                product_monthly = product_monthly[product_monthly['product_category'].str.split(', ').apply(lambda x: all(' - ' not in str(cat) for cat in x))]
            if not product_monthly.empty:
                fig_product_trend = px.line(product_monthly, x='month', y='count', color='product_category', title="ĞÑ‚Ğ·Ñ‹Ğ²Ñ‹ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ°Ğ¼ Ğ¿Ğ¾ Ğ¼ĞµÑÑÑ†Ğ°Ğ¼",
                                           height=600)
                st.plotly_chart(fig_product_trend, use_container_width=True)
            else:
                st.write("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸.")
        else:
            st.write("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸.")
    else:
        st.write("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ¸Ğ½Ğ°Ğ¼Ğ¸ĞºĞ¸.")

    # Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ²
    st.subheader("ğŸ“‹ Ğ Ğ°ÑĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¿Ğ¾ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸ÑĞ¼ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ²")
    if 'product_category' in filtered_df and not filtered_df['product_category'].isna().all():
        product_counts = filtered_df['product_category'].str.split(', ').explode().value_counts()
        if not product_counts.empty:
            if not product_filter or ('Ğ’ÑĞµ' in product_filter and len(product_filter) == 1):
                product_counts_filtered = product_counts[~product_counts.index.str.contains(' - ', na=False)]
            elif 'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸' in product_filter and len(product_filter) == 1:
                product_counts_filtered = product_counts[product_counts.index.isin(['ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸'] + subcategories_filter)]
            else:
                product_counts_filtered = product_counts[~product_counts.index.str.contains(' - ', na=False)]

            if not product_counts_filtered.empty:
                fig_product = px.bar(x=product_counts_filtered.index, y=product_counts_filtered.values, title="ĞšĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¸ Ğ¿Ñ€Ğ¾Ğ´ÑƒĞºÑ‚Ğ¾Ğ²",
                                    color=product_counts_filtered.index,
                                    color_discrete_map={
                                        'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸': '#1f77b4',
                                        'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸ - Ğ’ĞµĞ´ĞµĞ½Ğ¸Ğµ Ğ²Ğ°Ğ»ÑÑ‚Ğ½Ñ‹Ñ… ÑÑ‡ĞµÑ‚Ğ¾Ğ²': '#1f77b4',
                                        'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸ - Ğ”ĞµĞ±ĞµÑ‚Ğ¾Ğ²Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹': '#1f77b4',
                                        'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸ - ĞœĞ¾Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ±Ğ°Ğ½Ğº': '#1f77b4',
                                        'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸ - ĞŸĞµÑ€ĞµĞ²Ğ¾Ğ´Ñ‹': '#1f77b4',
                                        'ĞŸĞ¾Ğ²ÑĞµĞ´Ğ½ĞµĞ²Ğ½Ñ‹Ğµ Ñ„Ğ¸Ğ½Ğ°Ğ½ÑÑ‹ Ğ¸ Ğ¿Ğ»Ğ°Ñ‚ĞµĞ¶Ğ¸ - Ğ—Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğ½Ñ‹Ğµ ĞºĞ°Ñ€Ñ‚Ñ‹': '#1f77b4',
                                        'Ğ¡Ğ±ĞµÑ€ĞµĞ¶ĞµĞ½Ğ¸Ñ Ğ¸ Ğ½Ğ°ĞºĞ¾Ğ¿Ğ»ĞµĞ½Ğ¸Ñ': '#ff7f0e',
                                        'ĞšÑ€ĞµĞ´Ğ¸Ñ‚Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ': '#2ca02c',
                                        'Ğ˜Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ†Ğ¸Ğ¸': '#d62728',
                                        'Ğ¡Ñ‚Ñ€Ğ°Ñ…Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¸ Ğ·Ğ°Ñ‰Ğ¸Ñ‚Ğ°': '#9467bd',
                                        'ĞŸÑ€ĞµĞ¼Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑƒÑĞ»ÑƒĞ³Ğ¸': '#8c564b',
                                        'Ğ”Ñ€ÑƒĞ³Ğ¾Ğµ': '#bcbd22'
                                    },
                                    height=600)
                st.plotly_chart(fig_product, use_container_width=True)
            else:
                st.write("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹.")
        else:
            st.write("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ ĞºĞ°Ñ‚ĞµĞ³Ğ¾Ñ€Ğ¸Ğ¹.")

    # Ğ¢Ğ°Ğ±Ğ»Ğ¸Ñ†Ğ° Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ¾Ğ²
    st.subheader("ğŸ“ ĞŸĞ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ñ‹Ğµ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ñ‹")
    if not filtered_df.empty:
        group_cols_table = ['id', 'date', 'author', 'title', 'text', 'rating', 'sentiments', 'source', 'topics', 'product_category'] if all(col in filtered_df.columns for col in ['id', 'date', 'author', 'title', 'text', 'rating', 'sentiments', 'source', 'topics', 'product_category']) else ['date', 'author', 'title', 'text', 'rating', 'sentiments', 'source', 'topics', 'product_category']
        display_df = filtered_df.groupby(group_cols_table).size().reset_index(name='count')
        st.dataframe(display_df, width=1500, height=800)
    else:
        st.write("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ¾Ñ‚Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñ‹.")
else:
    st.write("ĞĞµÑ‚ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ… Ğ´Ğ»Ñ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°. Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ğ¸Ñ‚Ğµ CSV Ğ¸Ğ»Ğ¸ JSON Ñ Ğ¾Ñ‚Ğ·Ñ‹Ğ²Ğ°Ğ¼Ğ¸.")
